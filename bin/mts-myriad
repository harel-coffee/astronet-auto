#!/bin/bash -l
# Copyright 2021
# Author: Tarek Allam Jr.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Batch script to run a serial job under SGE.
# https://www.rc.ucl.ac.uk/docs/Job_Results/#qsub-emailing
#$ -m base

# Set up the job array.  In this instance we have requested 12 tasks
# numbered 1 to 12 for the 12 different MTS datasets.
#$ -t 1-12

# Request a number of GPU cards, in this case 2 (the maximum)
#$ -l gpu=1

# Request a V100 node only
#$ -ac allow=EF

# Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=47:10:0

# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)
#$ -l mem=80G

# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)
#$ -l tmpfs=15G

## #$ -o logs/$JOB_ID.log

# Combine stdout with stderr
#$ -j yes

# Set the name of the job.
#$ -N mts-GPU

# Set the working directory to somewhere in your scratch space.
#  This is a necessary step as compute nodes cannot write to $HOME.
# Replace "<your_UCL_id>" with your UCL user ID.
#$ -wd /home/zcicg57/Scratch/workspace

# Your work should be done in $TMPDIR
cd $TMPDIR

# load the cuda module (in case you are running a CUDA program)
module unload compilers mpi
module load compilers/gnu/4.9.2
module load python/3.7.4
module load cuda/10.0.130/gnu-4.9.2
# module load cuda/8.0.61-patch2/gnu-4.9.2
module load cudnn/7.4.2.24/cuda-10.0
# module load tensorflow/2.0.0/gpu-py37
# export LD_LIBRAY_PATH="/shared/ucl/apps/modulefiles/development/cuda/10.0.130/gnu-4.9.2:$LD_LIBRAY_PATH"

# Run the application
SECONDS=0 # https://stackoverflow.com/a/8903280/4521950
source /home/zcicg57/Scratch/astronet/conf/astronet.conf

export PATH="$HOME/miniconda3/envs/astronet/bin:$PATH"
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib # https://github.com/tensorflow/tensorflow/issues/52988#issue-1047548284
export CUDA_VISIBLE_DEVICES=0,1 # https://stackoverflow.com/a/48079860/4521950
# export TF_CPP_MIN_VLOG_LEVEL=3 # https://stackoverflow.com/q/66118532/4521950, https://stackoverflow.com/a/45142985/4521950
export TF_CPP_MIN_LOG_LEVEL=2
export TF_XLA_FLAGS=--tf_xla_enable_xla_devices # https://github.com/tensorflow/tensorflow/issues/46479
conda activate astronet
which python
export PYTHONHASHSEED=0
# Print the contents of this file to stdout from line 15 onwards
awk 'NR>15' $ASNWD/bin/mts-myriad
date
# Test Imports
python -c "import astronet as asn; print(asn.__version__)"
python -c "import tensorflow as tf; print(tf.__version__);"
python -c "import tensorflow as tf; print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))"
# Log GPU information
nvidia-smi

operation=$1 # i.e. {train, hyper}
# Architecture
## Current options: {t2, atx}
architecture=$2
echo "Running $1 using $architecture architecture"

declare -a arr=(
                "ArabicDigits"
                "AUSLAN"
                "CharacterTrajectories"
                "CMUsubject16"
                "ECG"
                "JapaneseVowels"
                "KickvsPunch"
                "Libras"
                "NetFlow"
                "UWave"
                "Wafer"
                "WalkvsRun"
            )

echo "${arr[SGE_TASK_ID-1]}"
dataset="${arr[SGE_TASK_ID-1]}"

if [ $architecture == "atx" ]; then
    if [ $operation == "hyper" ]; then
        # Hyperparameter Optimisation
        python $ASNWD/astronet/$architecture/opt/hypertrain.py \
            --dataset $dataset \
            --epochs 40
    elif [ $operation == "train" ]; then
        # Train
        python $ASNWD/astronet/$architecture/train.py \
            --dataset $dataset \
            --epochs 400
            # --model "scaledown-by-4"     # atx main model
    else
        echo "Please provide an argument for $operation"
    fi
elif [ $architecture == "t2" ]; then
    if [ $operation == "hyper" ]; then
        # Hyperparameter Optimisation
        python $ASNWD/astronet/$architecture/opt/hypertrain.py \
            --dataset $dataset \
            --epochs 40
    elif [ $operation == "train" ]; then
        # Train
        python $ASNWD/astronet/$architecture/train.py \
            --dataset $dataset \
            --epochs 400
    else
        echo "Please provide an argument for $operation"
    fi
else
    echo "Please provide an argument for $architecture"
fi
# Hyperparameter Optimisation
# python $ASNWD/astronet/$ARCH/opt/hypertrain.py --dataset $dataset --epochs 50
# Train
# python $ASNWD/astronet/$ARCH/train.py --dataset $dataset --epochs 400
date
duration=$SECONDS
echo "$(($duration / 60)) minutes and $(($duration % 60)) seconds elapsed."

# Preferably, tar-up (archive) all output files onto the shared scratch area
tar -zcvf $HOME/Scratch/files_from_job_$JOB_ID.tar.gz $TMPDIR
# Make sure you have given enough time for the copy to complete!
