{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Transformer [`t2`] (mini) Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Transformer Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Transformer is an attempt to apply the transformer architecture to time series. In this walk through I will use the **WISDM** (**WI**reless **S**ensor **D**ata **M**ining dataset: http://www.cis.fordham.edu/wisdm/includes/files/sensorKDD-2010.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this dataset is for multivariate time series classification, which is exactly the problem we are trying to solve for Type Ia Supernova. With Supernova classification we have observations accross 6 passbands for \"each timestep\". \n",
    "\n",
    "There is a ***disclaimer*** is since we don't _actually_ have measurements for all 6 'variables' for everytime step, but this can be resolved with a Gaussian Process regression for interpoloation of points between actual observed data.\n",
    "\n",
    "Therefore we will use this dataset as a proof on concept to see if this type of architecture would be suitable for our problem too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a short video showing how data of this type is collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XOEN9W05_4A\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the WISM dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `astronet/t2/utils.py` for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../astronet/t2/opt/hypertrain.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now inspect this hyperparameter optimisation run.\n",
    "\n",
    "We will first load in this study like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = f\"{unixtimestamp}-{label}\"\n",
    "with open(f\"{Path(__file__).absolute().parent}/runs/study-{study_name}.pkl\", \"rb\") as sf:\n",
    "    study = joblib.load(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load the best performing study out of all studies completed so far, one can run:\n",
    "# with open(f\"{Path(__file__).absolute().parent}/runs/results.json\") as f:\n",
    "#     events = json.load(f)\n",
    "#     event = max(events['optuna_result'], key=lambda ev: ev['value'])\n",
    "#     print(event)\n",
    "    \n",
    "# # Where study_name would be set via event['name'] like so:\n",
    "# study_name = event['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study, params=['embed_dim',\n",
    "                            'ff_dim', \n",
    "                            'num_heads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train on the best performing study of all hyperparamter optimsation studies. See above for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../astronet/t2/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../astronet/t2/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astronet.t2.visuals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log = t2_logger(__file__)\n",
    "    log.info(\"_________________________________\")\n",
    "    log.info(\"File Path:\" + str(Path(__file__).absolute()))\n",
    "    log.info(\"Parent of Directory Path:\" + str(Path().absolute().parent))\n",
    "except:\n",
    "    print(\"Seems you are running from a notebook...\")\n",
    "    __file__ = str(Path().resolve().parent) + \"/astronet/t2/visuals.py\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\"]})\n",
    "\n",
    "mpl.style.use(\"seaborn\")\n",
    "\n",
    "\n",
    "# Load WISDM-2010\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_WISDM()\n",
    "# One hot encode y\n",
    "enc, y_train, y_val, y_test = one_hot_encode(y_train, y_val, y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "with open(str(Path(__file__).absolute().parent) + '/models/results.json') as f:\n",
    "    events = json.load(f)\n",
    "    # Get params for best model with highest test accuracy\n",
    "    event = max(events['training_result'], key=lambda ev: ev['value'])\n",
    "    print(event)\n",
    "\n",
    "model_name = event['name']\n",
    "\n",
    "model = keras.models.load_model(str(Path(__file__).absolute().parent) + f\"/models/model-{model_name}\")\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(model_name, event, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "        model_name,\n",
    "        enc.inverse_transform(y_test),\n",
    "        enc.inverse_transform(y_pred),\n",
    "        enc.categories_[0], \n",
    "        save=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiROC(model_name, model, X_test, y_test, enc, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the [WISDM website](http://www.cis.fordham.edu/wisdm/dataset.php) the dataset can be describes as follows:\n",
    "    \n",
    "**Raw Time Series Data**\n",
    "- Number of examples: 1,098,207\n",
    "- Number of attributes: 6\n",
    "- Missing attribute values: None\n",
    "\n",
    "**Class Distribution**\n",
    "- Walking: 424,400 (38.6%)\n",
    "- Jogging: 342,177 (31.2%)\n",
    "- Upstairs: 122,869 (11.2%)\n",
    "- Downstairs: 100,427 (9.1%)\n",
    "- Sitting: 59,939 (5.5%)\n",
    "- Standing: 48,395 (4.4%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data\n",
    "\n",
    "It is important to scale features before training a neural network. Normalization is a common way of doing this scaling. Subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "In order to work with this data, we will need to do some pre-processing such that all axis are scaled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore there is another bit of \"pre-processing\" required such that we can use `keras` library for Deep Learning and classification. We need to split our dataset into a _train_ and _test_ (and _validation_) set and we will need to **one-hot** encode our targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets. With time-series data, it is very important not to allow for data to overlap from train to test, i.e we want to avoid the dreaded data leakage problem.\n",
    "\n",
    "For this we shall first split the data into a train/val/test set, and _then_ apply a windowing function to create small time segments we can then use for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now is a dataset of the shape above: (22454, 200, 3) (22454, 6) for `X_train` and `y_train` respectivly. \n",
    "\n",
    "This is to say we have 22452 _batches_, that are 200 time-step windows, that contain 3 variables each, i.e. `x, y, z` measurements. As you may have guess alreeady, this could be translated to X-number of _batches_, that are X-*MJD* day length windows, each containing 6 pass-band filter measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Transformer achitecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall architecture of the Transformer network can be seen below. It consists of the _encoder_ section and a _decoder_ section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/transformer-arch.png\" height=40% width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first instance, we will implement just the _encoder_ section show here and add a softmax output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/transformer-arch-encoder.png\" height=30% width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on results and if we are able to get the inputs to behave, we will move towards development and analysis of the full Transformer architecture for time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be seen pictorially as follows:\n",
    "<img src=\"./images/transformer-arch-multihead.png\" height=30% width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting the Transformer for Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer architeecture was originally applied to NLP tasks of translation. For this, there were specific stages in the pipeline that are focused towards working with words. For example, the embedding layer would project each word into a high-dimensional vector space followed by a positional enciding layer to ensure sequential information is preversed.\n",
    "\n",
    "In the case of time-series, a simple Dese layer can be used to map the (`m, T, 6`)(batch, time-steps, features) into a (`m, 32`)(batch, time-steps x features). In `keras`, in order to apply a Dense layer to each input in the time sequence, there is `TimeDistributed` layer.  \n",
    "\n",
    "From \"Hands-On Machine Learning\" by Aurelien Geron\n",
    "\n",
    "> The `TimeDistributed` layer wraps any layer (e.g. `Dense` layer) and applies it at every time step of its input sequence. It does this efficiently, by reshaping the inputs so that each time step is treated as a seperate instance ( i.e, it reshapes the inputs from [*batch size, time-steps, input dimension*] to [*batch size x time-steps, input dimensions].\n",
    "\n",
    "...\n",
    "> The `Dense` layer actually supports sequences as inputs (and even higher dimensional inputs): it handles them just like `TimeDistributed(Dense(...))`, meaning it is applied to the last input dimension only (independently accross all time steps). Thus, we could just replace ... with just `Dense(..)`\n",
    "\n",
    "> Note that a `TimeDistributed(Dense(n))` layer is equivalent to a `Conv1D(n, filter_size=1)` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://keras.io/api/layers/convolution_layers/convolution1d/\n",
    "* https://keras.io/api/layers/recurrent_layers/time_distributed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### But what is a 1-D convolition I hear you cry!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/vcp0XvDAX68\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to baseline models other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are at least comparable to other experimental results by [Jennifer R. Kwapisz et al 2010](https://www.cis.fordham.edu/wisdm/includes/files/sensorKDD-2010.pdf) where their best reported model of an MLP achieved acc ~ 91%. With notablly better confusion matrix on our part. See tables below:\n",
    "\n",
    "<img src=\"images/wisdm-2010-table2.png\" height=40% width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wisdm-2010-table5.png\" height=40% width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another piece of work that is work comparing against is the analysis done by Jason Brownlee found [here](https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scope of that analysis was to evaluate deep learning methods for a similiar HAR dataset, namely the [Human Activity Recognition Using Smartphones Data Set, UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included:\n",
    "\n",
    "> Pre-processing accelerometer and gyroscope using noise filters.\n",
    "Splitting data into fixed windows of 2.56 seconds (128 data points) with 50% overlap.Splitting of accelerometer data into gravitational (total) and body motion components.\n",
    "Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available.\n",
    "\n",
    "> A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features.\n",
    "\n",
    "> The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test.\n",
    "\n",
    "> Experiment results with a support vector machine intended for use on a smartphone (e.g. fixed-point arithmetic) resulted in a predictive accuracy of 89% on the test dataset, achieving similar results as an unmodified SVM implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jason Brownlee was able to get results on the order of ~ 90% for:\n",
    "- LSTM --> Accuracy: 89.722% (+/-1.371)\n",
    "- CNN+LSTM --> Accuracy: 90.689% (+/-1.051)\n",
    "- ConvLSTM --> Accuracy: 90.801% (+/-0.886)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is felt that with further training and better hyperparameter optimisation, better can be achieved, but it has been kept in mind that the cope of this analysis is proof of concept.\n",
    "\n",
    "As such, we will move on to exploring the updated version of **WISDM** released recently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WISDM-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In late 2019, an [updated **WISDM** dataset](https://archive.ics.uci.edu/ml/datasets/WISDM+Smartphone+and+Smartwatch+Activity+and+Biometrics+Dataset+) was release which included 18 categories with many more observations. It is a big imporovement on the previous dataset, with balanced class distributions shown in table 4 of the [paper](https://archive.ics.uci.edu/ml/machine-learning-databases/00507/WISDM-dataset-description.pdf) and visualised below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "#### Papers:\n",
    "\n",
    "\n",
    "#### Weblinks:\n",
    "\n",
    "* [The llustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "   1. Implement a custom callback\n",
    "   2. ~~Compare LSTM of blog post~~\n",
    "   3. Put code into t2 files\n",
    "   4. Explain dimensions, give examples. Windowing etc. use TF time_series.ipynb notebooks as example\n",
    "       - Build WindowGenerator, time_series.ipynb notebook for examples\n",
    "   5. Give in-depth explaination to adaption for time-series with relation to Transformer achitecture, use http://nlp.seas.harvard.edu/2018/04/03/attention.html and https://srome.github.io/Understanding-Attention-in-Neural-Networks-Mathematically/ as well as http://jalammar.github.io/illustrated-transformer/ and https://github.com/curiousily/Deep-Learning-For-Hackers/blob/master/13.time-series-human_activity_recognition.ipynb as reference\n",
    "   6. Add table of contents to this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
